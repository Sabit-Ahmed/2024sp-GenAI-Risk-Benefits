---
layout: post
title: Model editing and Disgorgement
lecture: 
lectureVersion: current
extraContent: 
notes: team-3
video: team-5
tags:
- Model Edit
---

In this session, our readings cover: 

## Required Readings: 

### Editing Large Language Models: Problems, Methods, and Opportunities
+ https://arxiv.org/abs/2305.13172
+ Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang
Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at this https URL.
Comments: EMNLP 2023. Updated with new experiments

## More Readings: 

### Tuning Language Models by Proxy
+ Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith
+ Submitted on 16 Jan 2024]
+ Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.



### A Survey of Machine Unlearning
  + https://arxiv.org/abs/2209.02299
  + Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at this https URL.



### AI Model Disgorgement: Methods and Choices
  + https://arxiv.org/abs/2304.03545
  + Alessandro Achille, Michael Kearns, Carson Klingenberg, Stefano Soatto
Responsible use of data is an indispensable part of any machine learning (ML) implementation. ML developers must carefully collect and curate their datasets, and document their provenance. They must also make sure to respect intellectual property rights, preserve individual privacy, and use data in an ethical way. Over the past few years, ML models have significantly increased in size and complexity. These models require a very large amount of data and compute capacity to train, to the extent that any defects in the training corpus cannot be trivially remedied by retraining the model from scratch. Despite sophisticated controls on training data and a significant amount of effort dedicated to ensuring that training corpora are properly composed, the sheer volume of data required for the models makes it challenging to manually inspect each datum comprising a training corpus. One potential fix for training corpus data defects is model disgorgement -- the elimination of not just the improperly used data, but also the effects of improperly used data on any component of an ML model. Model disgorgement techniques can be used to address a wide range of issues, such as reducing bias or toxicity, increasing fidelity, and ensuring responsible usage of intellectual property. In this paper, we introduce a taxonomy of possible disgorgement methods that are applicable to modern ML systems. In particular, we investigate the meaning of "removing the effects" of data in the trained model in a way that does not require retraining from scratch.

### Outline
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide2.PNG" width="80%" height="80%">

- The presenters discussed 3 primary topics:
1. Editing Large Language Models
2. Tuning Language Models by Proxy
3. A survey of Machine Unlearning

## Paper 1: Editing Large Language Models
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide3.PNG" width="80%" height="80%">

### Context
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide5.PNG" width="80%" height="80%">
As is visible from the graph, LLMs have seen a meteoric rise in recent times. This graph relates the number of parameters in models to time, by year since 2020. It also shows which models are available with open access, and shows larger circles for models with more parameters. 

### Unwanted Knowledge
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide6.PNG" width="80%" height="80%">
LLMs can easily learn unwanted knowledge. If given poor input data, it can output biased responses. The authors will discuss if there is an efficient way for large language models to update their knowledge.

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide7.PNG" width="80%" height="80%">
Editing LLMs is necessary because the world changes after they are released. Labels shift, and the ground truth for their answers can shift as well. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide8.PNG" width="80%" height="80%">
The authors discuss 3 primary ways of updating a model:
1. Fine-tuning: drawbacks include its computational requirements and how easy it is to overfit.
2. Retrieval augmented: can scale poorly and suffer from retrieval noise
3. Model editing: gives precise control, but can be difficult and ineffective.

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide9.PNG" width="80%" height="80%">
In this slide the presenters formally describe the task at hand. The goal is to modify a model's behavior for one particular edit descriptor while leaving other behaviors unchanged. The edit scope is also formally defined with *S*, and behaviors can either be in-scope or out-of-scope. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide10.PNG" width="80%" height="80%">
For evaluation, the authors primarily use metrics of reliability, generalization, and locality.

#### Current Methods
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide12.PNG" width="80%" height="80%">
This slide shows how current methods could be used to modify an edit descriptor in a model. The upper section shows a method to modify the behavior while preserving the model's parameters. The lower section shows a method wherein the model's parameters are modified. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide13.PNG" width="80%" height="80%">
The authors present this table to compare the current methods and specify additional attributes of their approaches. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide15.PNG" width="80%" height="80%">
The authors now experiment with the different approaches. Their experiments are based on factual knowledge, which is information that can be verified as true or false based on empirical evidence or authoritative sources. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide16.PNG" width="80%" height="80%">
The authors will utilize the CounterFact dataset to measure the efficacy of significant changes. This slide also shows the composition of that dataset.

#### Experimental Results
<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide17.PNG" width="80%" height="80%">
This slide shows the results of existing methods on three metrics of the dataset: reliability, generalization, and locality. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide18.PNG" width="80%" height="80%">
In terms of scaling, the authors note that the ROME and MEMEIT approaches perform well on the GPT-NEOX-20B model but fail on OPT-13B. They note that large amounts of matrix computations and in-context learning ability could limit the efficacy of certain approaches. 

<img src="{{ site.baseurl }}/Lectures/S0-L17/Slide19.PNG" width="80%" height="80%">
Batch editing is required to modify a model with multiple knowledge pieces simultaneously. Some methods are batch-editing-supportive. Figure 3 shows batch editing performance vs. batch number. MEMEIT appears to be one of the best approaches in this regard. 