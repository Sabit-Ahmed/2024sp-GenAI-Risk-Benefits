---
layout: post
title: Prompt Engineering 
lecture: 
lectureVersion: current
extraContent: 
notes: team-6
video: team-2
tags:
- APE
desc: 2024-S20
term: 2024-seminarRead
categories:
- FMAdapt
---


In this session, our readings cover: 

## Required Readings: 

### Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review
  + https://arxiv.org/abs/2310.14735
  + Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, Shengxin Zhu / This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.


## More Readings: 

### Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding
+ This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and further underscores the potential of pushing LLMs to think more like a human for answer quality.

### Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts
+ The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and others parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.




  
### Long context prompting for Claude 2.1
+ https://www.anthropic.com/news/claude-2-1-prompting


# Skeleton Of Thought: Prompting LLMs For Efficient Parallel Generation
## Motivation
LLMs have powerful performance, but the inference speed is low due to :

- Large model size
- Expensive attention operation
- The sequential decoding approach

Existing work either compress/redesign the model, serving system, hardware.

This work instead focus on **the 3rd axis** and propose **Skeleton Of Thought for efficient parallel decoding** without **any changes to LLM models, systems and hardwares.**

## High-level Overview

The idea comes from how humans answer questions. Steps of human thoughts can be summarized as below:
1.  Derive out the skeleton according to protocals and strategies.
2. Add evidence and details to explain each point.
If we visualize these steps, it looks like:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/01_human_thoughts.png" width="50%" height="50%"></p>

Based on this, this paper proposed **Skeleton-of-Thought** as shown in Figure below which includes 3 steps:
1. Prompt the LLM to give out the skeleton.
2. Conduct batched decoding or parallel API calls to expand multiple points in parallel.
3. Aggregate the outputs to get final answer.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/02_SoT.png" width="80%" height="80%"></p>

Compared with 12 recently released LLMs, SoT can not only provide considerable speed-ups but also improve the answer quality as shown in figure below. 

The y-axis `net win rate` is the difference between the fraction of questions that SoT-R has better and worse answers than normal generation.

The x-axis `speed-up` is the ratio between the latency of normal and SoT-R generation.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/03_SoT_Comparison.png" width="80%" height="80%"></p>

## Method
The method of SoT has two stages: `skeleton stage` and `point-expanding stage`.

### Skeleton Stage
In skeleton stage, SoT uses a skeleton prompt to guide the LLM to output a concise skeleton of the answer so that we can extract some points from the skeleton response. A prompt example is shown in Figure below.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/04_Skeleton_Prompt.png" width="80%" height="80%"></p>

### Point-expanding Stage
Based on the skeleton, SoT uses point-expanding prompt to let LLM expand on each point in parallel. A prompt example is shown in Figure below. After completing all points, SoT concatenate all the point-expanding responses to get the final answer.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/05_Point_Expanding_Prompt.png" width="80%" height="80%"></p>


### Parallelization
The authors use parallel point expanding to achieve speed-up than normal decoding. In specific:
- For proprietary models with only API access, parallelization is achieved by issuing multiple API calls.
- For open-source models that we can run locally, parallelization is achieved by letting LLMs process point-expanding requests as a batch.

## Evaluation – Overall Quality

For the evaluation, we can assess it from various perspectives.

- **Evaluation Process​:**

  - Present a question and a pair of answers to an LLM judge.

- **LLM-based evaluation frameworks:**

  - FastChat: general metric.

  - LLMZoo: general metric plus 5 detailed metrics - coherence, diversity, immersion, integrity, and relevance.

- **Extensions to avoid evaluation bias:**

  - Running the evaluation twice with either ordering of the two answers

  - For each run, a score is assigned: 1 – win; 0 – tie; -1 – lose

  - Sum the two scores to get the final score

- **Net win rates:**

  - (#win - #lose)/total number of questions

## Evaluation – Evaluation of Answer Quality

- **Regarding Overall Quality, based on the figure provided, we can conclude:**

  - There is a discrepancy between the two metrics on win rates.

  - SoT is not worse than the baseline in around 60% of the cases.

  - The lose rates are also pretty high.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/1.png" width="80%" height="80%"></p>

- **Regarding the quality of each model, the conclusions drawn from the figure indicate:**

  - The red rectangular frame in the figure highlights: Both metrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B have **negative net win rates.**

  - The green rectangular frame in the figure highlights: Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B have **positive net win rates.**

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/2.png" width="80%" height="80%"></p>

- **Based on the figure, the reasons for bad net win rates can be identified as follows:**

The question and answer provided by OpenChat-13B in the figure demonstrate that models construct the complete answer during the skeleton stage. And the figure showing the question and answer from Vicuna-7B V1.1 illustrates that models omit details during the point-expanding stage.

In summary, some strong models have very high-quality answers that are hard to beat.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/3.png" width="80%" height="80%"></p>

- **Regarding the quality of each question category, our conclusions from the figure are:**

  - The green rectangular frame in the figure highlights: SoT performs relatively well on generic, common-sense, knowledge, and counterfactual questions.

  - The red rectangular frame in the figure highlights: Relatively poorly on writing, fermi, math, and coding.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/4.png" width="80%" height="80%"></p>

- **Concerning the Quality of Detailed Metrics, the information from the figure reveals:**

  - SoT improves the diversity and relevance while hurting the immersion and coherence.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/5.png" width="80%" height="80%"></p>

### SoT-R – Definition and Framework

- **Prompting Router:**

  - Ask the LLM if the desired answer is in a list of independent points.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/6.png" width="80%" height="80%"></p>

- **Trained Router:**

  - **Annotate** the LIMA training set: a label of 1 or 0.

  - **Fine-tune** a RoBERTa model using the labeled data.

  - Ask the RoBERTa to **classify** if the SoT is suitable for the desired answer.

## ​SoT-R – Evaluation

Based on the provided figures, we can understand:

- SoT-R obtains **lower speed-ups** than SoT.

- SoT-R significantly **improves the answer quality** on questions where SoT is not suitable.

- The two types of SoT-R perform similarly to a human router.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/7.png" width="80%" height="80%"></p>

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L20/images/Skeleton Of Thought/8.png" width="80%" height="80%"></p>

## Conclusion 

Having thoroughly reviewed the paper, we've gained significant insights into the Skeleton of Thought concept. From this, we can derive several conclusions, each from a unique perspective:

- **Efficient LLM methods at model and system levels:**

  - SoT is a **data-level** technique.

- **Prompting methods for LLMs:**

  - SoT is the first attempt at exploiting the **power of prompting to improve efficiency.**

- **Answer quality evaluation:**

  - The answer quality evaluation is far from perfect due to the limited prompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM generations.

- **Efficiency and overhead of SoT in different scenarios:**

  - **higher costs** due to the increased number of API calls and tokens.
  - **computation overhead**

- **Eliciting or improving LLMs’ ability:**
  - Graph-of-Thoughts

