---
layout: post
title: LLM interpretibility 
lecture: 
lectureVersion: next
extraContent: 
notes: team-4
video: team-6
tags:
- 1Basic
---

## Required Readings: 


#### Rethinking interpretability in the era of large language models
+ Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao
+ 2024/1/30
+ Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.

#### The Claude 3 Model Family: Opus, Sonnet, Haiku
+ https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf
+ We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed,
and Claude 3 Haiku, our fastest and least expensive model. All new models have vision
capabilities that enable them to process and analyze image data. The Claude 3 family
demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results
on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku
performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and
Opus significantly outperform it. Additionally, these models exhibit improved fluency in
non-English languages, making them more versatile for a global audience. In this report,
we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety,
societal impacts, and the catastrophic risk assessments we committed to in our Responsible
Scaling Policy [5].

## More Readings: 

#### Transformer Debugger
+ https://github.com/openai/transformer-debugger
+ Transformer Debugger (TDB) is a tool developed by OpenAI's Superalignment team with the goal of supporting investigations into specific behaviors of small language models. The tool combines automated interpretability techniques with sparse autoencoders. TDB enables rapid exploration before needing to write code, with the ability to intervene in the forward pass and see how it affects a particular behavior. It can be used to answer questions like, "Why does the model output token A instead of token B for this prompt?" or "Why does attention head H attend to token T for this prompt?" It does so by identifying specific components (neurons, attention heads, autoencoder latents) that contribute to the behavior, showing automatically generated explanations of what causes those components to activate most strongly, and tracing connections between components to help discover circuits.



#### Towards Monosemanticity: Decomposing Language Models With Dictionary Learning
+ https://transformer-circuits.pub/2023/monosemantic-features/index.html
+ In this paper, we use a weak dictionary learning algorithm called a sparse autoencoder to generate learned features from a trained model that offer a more monosemantic unit of analysis than the model's neurons themselves. Our approach here builds on a significant amount of prior work, especially in using dictionary learning and related methods on neural network activations , and a more general allied literature on disentanglement. We also note interim reports  which independently investigated the sparse autoencoder approach in response to Toy Models, culminating in the recent manuscript of Cunningham et al. 
+ related post: Decomposing Language Models Into Understandable Components https://www.anthropic.com/news/decomposing-language-models-into-understandable-components



#### Tracing Model Outputs to the Training Data
+ https://www.anthropic.com/news/influence-functions
+ As large language models become more powerful and their risks become clearer, there is increasing value to figuring out what makes them tick. In our previous work, we have found that large language models change along many personality and behavioral dimensions as a function of both scale and the amount of fine-tuning. Understanding these changes requires seeing how models work, for instance to determine if a model’s outputs rely on memorization or more sophisticated processing. Understanding the inner workings of language models will have substantial implications for forecasting AI capabilities as well as for approaches to aligning AI systems with human preferences.
Mechanistic interpretability takes a bottom-up approach to understanding ML models: understanding in detail the behavior of individual units or small-scale circuits such as induction heads. But we also see value in a top-down approach, starting with a model’s observable behaviors and generalization patterns and digging down to see what neurons and circuits are responsible. An advantage of working top-down is that we can directly study high-level cognitive phenomena of interest which only arise at a large scale, such as reasoning and role-playing. Eventually, the two approaches should meet in the middle.



#### Language models can explain neurons in language models
+ https://openai.com/research/language-models-can-explain-neurons-in-language-models
+ Language models have become more capable and more widely deployed, but we do not understand how they work. Recent work has made progress on understanding a small number of circuits and narrow behaviors,[1][2]  but to fully understand a language model, we'll need to analyze millions of neurons. This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability [3][4][5]  will enable us to comprehensively audit the safety of models before deployment.

# Session Blog
## Rethinking Interpretability in the Era of Large Language Models
Section based on the paper [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761)
+ In traditional ML interpretability,
    + Building inherently interpretable models,
        + such as sparse linear models and decision trees
    + Post-hoc interpretability techniques
        + Such as Grad-Cam that relies on saliency maps
+ A new opportunity in LLM interpretability:
    + Explanation Generation
    + “Can you explain your logic?” “ Why didn’t you answer with (A)?”

Interpretability Definition:
Extraction of relevant knowledge concerning relationships contained in data or learned by the model
The definition applies to both:
1. Interpreting an LLM, and 
2. Using an LLM to generate explanations

Breakdown of LLM interpretability: Uses and Themes
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/Interpretability_diagram_1.PNG" width="100%" height="100%">

Description example
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/Interpretability_attack_2.PNG" width="100%" height="100%">

### Local Explanation
Explain a Single Generation by Token-level Attributions
+ Providing feature attributions for input tokens
    + perturbation-based methods
    + gradient-based methods
    + linear approximations
+ Attention mechanisms for visualizing token contribution to a generation
+ LLM can generate post-hoc feature attributions by prompting

Post-hoc feature attributions by prompting LLM
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/Interpretability_prompt_3.PNG" width="100%" height="100%">

Explain a Single Generation Directly in Natural Language
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/explain_text_4.PNG" width="100%" height="100%">

Challenges: Hallucination
Mitigation: 
+ Generate explanation within the answer: 
    + Chain-of-thought prompting
    + Tree-of-thoughts 
+ Retrieval Augmented Generation

### Global Explanation
#### Probing
Analyze the model’s representation by decoding its embedded information
Probing can apply to 
+ Attention heads
+ Embeddings
+ Different controllable representations

Probing as it applies to text embeddings:
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/probing_5.PNG" width="100%" height="100%">

More Granular Level Representation
+ categorizing or decoding concepts from individual neurons
+ explaining the function of attention heads in natural language

How groups of neurons combine to perform specific tasks
+ finding a circuit for indirect object identification
+ entity binding

#### GPT-4 Probing Example
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/gpt4_explain_6.PNG" width="100%" height="100%">
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/NN_explain_7.PNG" width="100%" height="100%">
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/gpt4_explain_8.PNG" width="100%" height="100%">
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/gpt4_explain_9.PNG" width="100%" height="100%">

### Dataset Explanation
Data set explanation occurs along a spectrum of low-high level techniques:
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/dataset_scale_10.PNG" width="100%" height="100%">

Text Data
Using LLM to build interpretable Linear Models / Decision Trees. Basically just using LLMs to summarize details of less interpretable models.
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/dataset_llm_11.PNG" width="100%" height="100%">
Partially interpretable models via chain of prompts techniques:
<img src="{{ site.baseurl }}/Lectures/S0-L18/images/prompt_chain_12.PNG" width="100%" height="100%">

### Future Directions
Explanation reliability: prevent hallucinations from leaking in to explanations, ensure that explanations are related to the actual process of the model if asking it to explain itself, implement some kind of verification techniques.
Dataset explanation for knowledge discovery: better usages of models to summarize, create and display statistics, and extract knowledge from datasets
Interactive explanations: make the process more dynamic and accessible

## Claude Model 3 Family


